# ğŸ¤£ KoBART

* [ğŸ¤£ KoBART](#-kobart)
  * [How to install](#how-to-install)
  * [Data](#data)
  * [Tokenizer](#tokenizer)
  * [Model](#model)
    * [Performances](#performances)
      * [Classification or Regression](#classification-or-regression)
      * [Summarization](#summarization)
  * [Demos](#demos)
  * [Examples](#examples)
  * [Release](#release)
  * [Contacts](#contacts)
  * [License](#license)

[**BART**](https://arxiv.org/pdf/1910.13461.pdf)(**B**idirectional and **A**uto-**R**egressive **T**ransformers)ëŠ” ì…ë ¥ í…ìŠ¤íŠ¸ ì¼ë¶€ì— ë…¸ì´ì¦ˆë¥¼ ì¶”ê°€í•˜ì—¬ ì´ë¥¼ ë‹¤ì‹œ ì›ë¬¸ìœ¼ë¡œ ë³µêµ¬í•˜ëŠ” `autoencoder`ì˜ í˜•íƒœë¡œ í•™ìŠµì´ ë©ë‹ˆë‹¤. í•œêµ­ì–´ BART(ì´í•˜ **KoBART**) ëŠ” ë…¼ë¬¸ì—ì„œ ì‚¬ìš©ëœ `Text Infilling` ë…¸ì´ì¦ˆ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ **40GB** ì´ìƒì˜ í•œêµ­ì–´ í…ìŠ¤íŠ¸ì— ëŒ€í•´ì„œ í•™ìŠµí•œ í•œêµ­ì–´ `encoder-decoder` ì–¸ì–´ ëª¨ë¸ì…ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ë„ì¶œëœ `KoBART-base`ë¥¼ ë°°í¬í•©ë‹ˆë‹¤.

![bart](imgs/bart.png)

## How to install

```bash
pip install git+https://github.com/SKT-AI/KoBART#egg=kobart
```

## Data

| Data         | # of Sentences |
| ------------ | -------------: |
| Korean Wiki  |             5M |
| Other corpus |          0.27B |

í•œêµ­ì–´ ìœ„í‚¤ ë°±ê³¼ ì´ì™¸, ë‰´ìŠ¤, ì±…, [ëª¨ë‘ì˜ ë§ë­‰ì¹˜ v1.0(ëŒ€í™”, ë‰´ìŠ¤, ...)](https://corpus.korean.go.kr/), [ì²­ì™€ëŒ€ êµ­ë¯¼ì²­ì›](https://github.com/akngs/petitions) ë“±ì˜ ë‹¤ì–‘í•œ ë°ì´í„°ê°€ ëª¨ë¸ í•™ìŠµì— ì‚¬ìš©ë˜ì—ˆìŠµë‹ˆë‹¤.

## Tokenizer

[`tokenizers`](https://github.com/huggingface/tokenizers) íŒ¨í‚¤ì§€ì˜ `Character BPE tokenizer`ë¡œ í•™ìŠµë˜ì—ˆìŠµë‹ˆë‹¤.

`vocab` ì‚¬ì´ì¦ˆëŠ” 30,000 ì´ë©° ëŒ€í™”ì— ìì£¼ ì“°ì´ëŠ” ì•„ë˜ì™€ ê°™ì€ ì´ëª¨í‹°ì½˜, ì´ëª¨ì§€ ë“±ì„ ì¶”ê°€í•˜ì—¬ í•´ë‹¹ í† í°ì˜ ì¸ì‹ ëŠ¥ë ¥ì„ ì˜¬ë ¸ìŠµë‹ˆë‹¤.
> ğŸ˜€, ğŸ˜, ğŸ˜†, ğŸ˜…, ğŸ¤£, .. , `:-)`, `:)`, `-)`, `(-:`...

ë˜í•œ `<unused0>` ~ `<unused99>`ë“±ì˜ ë¯¸ì‚¬ìš© í† í°ì„ ì •ì˜í•´, í•„ìš”í•œ `subtasks`ì— ë”°ë¼ ììœ ë¡­ê²Œ ì •ì˜í•´ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ í–ˆìŠµë‹ˆë‹¤.

```python
>>> from kobart import get_kobart_tokenizer
>>> kobart_tokenizer = get_kobart_tokenizer()
>>> kobart_tokenizer.tokenize("ì•ˆë…•í•˜ì„¸ìš”. í•œêµ­ì–´ BART ì…ë‹ˆë‹¤.ğŸ¤£:)l^o")
['â–ì•ˆë…•í•˜', 'ì„¸ìš”.', 'â–í•œêµ­ì–´', 'â–B', 'A', 'R', 'T', 'â–ì…', 'ë‹ˆë‹¤.', 'ğŸ¤£', ':)', 'l^o']
```

## Model

| Model         | # of params |  Type   | # of layers | # of heads | ffn_dim | hidden_dims |
| ------------- | :---------: | :-----: | ----------: | ---------: | ------: | ----------: |
| `KoBART-base` |    124M     | Encoder |           6 |         16 |    3072 |         768 |
|               |             | Decoder |           6 |         16 |    3072 |         768 |

```python
>>> from transformers import BartModel
>>> from kobart import get_pytorch_kobart_model, get_kobart_tokenizer
>>> kobart_tokenizer = get_kobart_tokenizer()
>>> model = BartModel.from_pretrained(get_pytorch_kobart_model())
>>> inputs = kobart_tokenizer(['ì•ˆë…•í•˜ì„¸ìš”.'], return_tensors='pt')
>>> model(inputs['input_ids'])
Seq2SeqModelOutput(last_hidden_state=tensor([[[-0.4418, -4.3673,  3.2404,  ...,  5.8832,  4.0629,  3.5540],
         [-0.1316, -4.6446,  2.5955,  ...,  6.0093,  2.7467,  3.0007]]],
       grad_fn=<NativeLayerNormBackward>), past_key_values=((tensor([[[[-9.7980e-02, -6.6584e-01, -1.8089e+00,  ...,  9.6023e-01, -1.8818e-01, -1.3252e+00],
```

### Performances

#### Classification or Regression

|                 | [NSMC](https://github.com/e9t/nsmc)(acc) | [KorSTS](https://github.com/kakaobrain/KorNLUDatasets)(spearman) | [Question Pair](https://github.com/aisolab/nlp_classification/tree/master/BERT_pairwise_text_classification/qpair)(acc) |
| --------------- | ---------------------------------------- | ---------------------------------------------------------------- | ------------------------------------------------------------------------------------
----------------------------------- |
| **KoBART-base** | 90.24                                    | 81.66                                                            | 94.34                                                                                                                   |

#### Summarization

* ì—…ë°ì´íŠ¸ ì˜ˆì • *

## Demos

* [ìš”ì•½ ë°ëª¨](https://huggingface.co/spaces/gogamza/kobart-summarization)

<img src="imgs/kobart_summ.png" width="600"/>

*ìœ„ ì˜ˆì‹œëŠ” [ZDNET ê¸°ì‚¬](https://zdnet.co.kr/view/?no=20201125093328)ë¥¼ ìš”ì•½í•œ ê²°ê³¼ì„*

## Examples

* [NSMC Classification](https://github.com/SKT-AI/KoBART/tree/main/examples)
* [KoBART ChitChatBot](https://github.com/haven-jeon/KoBART-chatbot)
* [KoBART Summarization](https://github.com/seujung/KoBART-summarization)
* [KoBART Translation](https://github.com/seujung/KoBART-translation)
* [LegalQA using Sentence**KoBART**](https://github.com/haven-jeon/LegalQA)
* [KoBART Question Generation](https://github.com/Seoneun/KoBART-Question-Generation)

*KoBARTë¥¼ ì‚¬ìš©í•œ í¥ë¯¸ë¡œìš´ ì˜ˆì œê°€ ìˆë‹¤ë©´ PRì£¼ì„¸ìš”!*

## Release

* v0.5.1
  * guide default 'import statements'
* v0.5
  * download large files from `aws s3`
* v0.4
  * Update model binary
* v0.3
  * í† í¬ë‚˜ì´ì € ë²„ê·¸ë¡œ ì¸í•´ `<unk>` í† í°ì´ ì‚¬ë¼ì§€ëŠ” ì´ìŠˆ í•´ê²°
* v0.2
  * `KoBART` ëª¨ë¸ ì—…ë°ì´íŠ¸(ì„œë¸Œí…ŒìŠ¤íŠ¸ sample efficientê°€ ì¢‹ì•„ì§)
  * `ëª¨ë‘ì˜ ë§ë­‰ì¹˜` ì‚¬ìš© ë²„ì „ ëª…ì‹œ
  * downloder ë²„ê·¸ ìˆ˜ì •
  * `pip` ì„¤ì¹˜ ì§€ì›

## Contacts

`KoBART` ê´€ë ¨ ì´ìŠˆëŠ” [ì´ê³³](https://github.com/SKT-AI/KoBART/issues)ì— ì˜¬ë ¤ì£¼ì„¸ìš”.

## License

`KoBART`ëŠ” `modified MIT` ë¼ì´ì„ ìŠ¤ í•˜ì— ê³µê°œë˜ì–´ ìˆìŠµë‹ˆë‹¤. ëª¨ë¸ ë° ì½”ë“œë¥¼ ì‚¬ìš©í•  ê²½ìš° ë¼ì´ì„ ìŠ¤ ë‚´ìš©ì„ ì¤€ìˆ˜í•´ì£¼ì„¸ìš”. ë¼ì´ì„ ìŠ¤ ì „ë¬¸ì€ `LICENSE` íŒŒì¼ì—ì„œ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
