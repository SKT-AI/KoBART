
<!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList=false} -->

<!-- code_chunk_output -->

- [ğŸ¤£ KoBART](#kobart)
  - [How to install](#how-to-install)
  - [Data](#data)
  - [Tokenizer](#tokenizer)
  - [Model](#model)
    - [Performances](#performances)
  - [Contacts](#contacts)
  - [License](#license)

<!-- /code_chunk_output -->


# ğŸ¤£ KoBART

[**BART**](https://arxiv.org/pdf/1910.13461.pdf)(**B**idirectional and **A**uto-**R**egressive **T**ransformers)ëŠ” ì…ë ¥ í…ìŠ¤íŠ¸ ì¼ë¶€ì— ë…¸ì´ì¦ˆë¥¼ ì¶”ê°€í•˜ì—¬ ì´ë¥¼ ë‹¤ì‹œ ì›ë¬¸ìœ¼ë¡œ ë³µêµ¬í•˜ëŠ” `autoencoder`ì˜ í˜•íƒœë¡œ í•™ìŠµì´ ë©ë‹ˆë‹¤. í•œêµ­ì–´ BART(ì´í•˜ **KoBART**) ë…¼ë¬¸ì—ì„œ ì–¸ê¸‰í•œ `Text Infilling` ë…¸ì´ì¦ˆ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ **40GB** ì´ìƒì˜ í•œêµ­ì–´ í…ìŠ¤íŠ¸ì— ëŒ€í•´ì„œ í•™ìŠµí•œ í•œêµ­ì–´ `encoder-docoder` ì–¸ì–´ ëª¨ë¸ì…ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ë„ì¶œëœ `KoBART-base`ì„ ë°°í¬í•©ë‹ˆë‹¤.


![](imgs/bart.png)

## How to install

```
git clone https://github.com/SKT-AI/KoBART.git
cd KoBART
pip install -r requirements.txt
pip install .
```

## Data

| Data  | # of Sentences |
|-------|---------------:|
| Korean Wiki |     5M   |  
| Other corpus |  0.27B    | 

í•œêµ­ì–´ wiki ë°ì´í„° ì´ì™¸ ë‰´ìŠ¤, books, ëª¨ë‘ì˜ ì½”í¼ìŠ¤(ëŒ€í™”, ë‰´ìŠ¤, ...), ì²­ì™€ëŒ€ êµ­ë¯¼ì²­ì› ë“±ì˜ ë‹¤ì–‘í•œ ë°ì´í„°ê°€ ì‚¬ìš©ë˜ì—ˆìŠµë‹ˆë‹¤.

## Tokenizer

[`tokenizers`](https://github.com/huggingface/tokenizers) íŒ¨í‚¤ì§€ì˜ `Character BPE tokenizer`ë¡œ í•™ìŠµë˜ì—ˆìŠµë‹ˆë‹¤. 

`vocab` ì‚¬ì´ì¦ˆëŠ” 30,000 ì´ë©° ëŒ€í™”ì— ìì£¼ ì“°ì´ëŠ” ì•„ë˜ì™€ ê°™ì€ ì´ëª¨í‹°ì½˜, ì´ëª¨ì§€ ë“±ì„ ì¶”ê°€í•˜ì—¬ í•´ë‹¹ í† í°ì˜ ì¸ì‹ ëŠ¥ë ¥ì„ ì˜¬ë ¸ìŠµë‹ˆë‹¤. 
> ğŸ˜€, ğŸ˜, ğŸ˜†, ğŸ˜…, ğŸ¤£, ,..., `:-)`, `:)`, `-)`, `(-:`...

ë˜í•œ `<unused0>` ~ `<unused99>`ë“±ì˜ ë¯¸ì‚¬ìš© í† í°ì„ ì •ì˜í•´ í•„ìš”í•œ `subtasks`ì— ë”°ë¼ ììœ ë¡­ê²Œ ì •ì˜í•´ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ í–ˆìŠµë‹ˆë‹¤.


```python
>>> from kobart import get_kobart_tokenizer
>>> kobart_tokenizer = get_kobart_tokenizer()
>>> kobart_tokenizer.tokenize("ì•ˆë…•í•˜ì„¸ìš”. í•œêµ­ì–´ BART ì…ë‹ˆë‹¤.")
['â–ì•ˆë…•í•˜', 'ì„¸ìš”.', 'â–í•œêµ­ì–´', 'â–B', 'A', 'R', 'T', 'â–ì…', 'ë‹ˆë‹¤.']
```

## Model

| Model         |  Type   | n_layers  | n_heads | ffn_dim | hidden_dims | 
|---------------|:-------:|--------:|--------:|--------:|--------------:|
| `KoBART-base` | Encoder |   6     | 16      | 3072    | 768           |
|               | Decoder |   6     | 16      | 3072    | 768           |


```python
>>> from transformers import BartModel
>>> from kobart import get_pytorch_kobart_model, get_kobart_tokenizer
>>> kobart_tokenizer = get_kobart_tokenizer()
>>> model = BartModel.from_pretrained(get_pytorch_kobart_model())
>>> inputs = kobart_tokenizer(['ì•ˆë…•í•˜ì„¸ìš”.'], return_tensors='pt')
>>> model(inputs['input_ids'])
Seq2SeqModelOutput(last_hidden_state=tensor([[[-1.5372, -2.5599,  0.8382,  ..., -2.6832,  2.5374,  1.7316],
         [-1.6075, -3.0245,  1.3806,  ..., -3.4531,  1.8102,  2.0583]]],
       grad_fn=<TransposeBackward0>), past_key_values=None, decoder_hidden_states=None, decoder_attentions=None, cross_attentions=None, encoder_last_hidden_state=tensor([[[ 0.5163, -0.3525,  0.5279,  ...,  0.1081,  0.5969,  0.1189],
         [ 0.4078, -0.3281,  0.6627,  ...,  0.0751,  0.6414,  0.3749]]],
       grad_fn=<TransposeBackward0>), encoder_hidden_states=None, encoder_attentions=None) 
```

### Performances

|   |  NSMC(acc)  | KorSTS(spearman) | Question Pair(acc) | 
|---|---|---|---|
| **KoBART**  | 90.07  | 81.31  | 93.80  |


## Contacts

`KoBART` ê´€ë ¨ ì´ìŠˆëŠ” [ì´ê³³](https://github.com/SKT-AI/KoBART/issues)ì— ì˜¬ë ¤ì£¼ì„¸ìš”.

## License

`KoBART`ëŠ” `modified MIT` ë¼ì´ì„ ìŠ¤ í•˜ì— ê³µê°œë˜ì–´ ìˆìŠµë‹ˆë‹¤. ëª¨ë¸ ë° ì½”ë“œë¥¼ ì‚¬ìš©í•  ê²½ìš° ë¼ì´ì„ ìŠ¤ ë‚´ìš©ì„ ì¤€ìˆ˜í•´ì£¼ì„¸ìš”. ë¼ì´ì„ ìŠ¤ ì „ë¬¸ì€ `LICENSE` íŒŒì¼ì—ì„œ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
